{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda280e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    get_linear_schedule_with_warmup, \n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "import torch\n",
    "from tqdm import trange, tqdm\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, contexts, labels):\n",
    "        self.contexts = contexts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx], self.labels[idx]\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "conan = pd.read_csv(\"/data/shared/hate_speech_dataset/CONAN.csv\")\n",
    "\n",
    "hate_df = conan[['hateSpeech']].drop_duplicates()\n",
    "hate_df = hate_df.rename(columns={'hateSpeech': 'sentence'})\n",
    "hate_df = hate_df.reset_index(drop=True)\n",
    "hate_df['label'] = 1\n",
    "\n",
    "nonhate_df = conan[['counterSpeech']].drop_duplicates()\n",
    "nonhate_df = nonhate_df.rename(columns={'counterSpeech': 'sentence'})\n",
    "nonhate_df = nonhate_df.reset_index(drop=True)\n",
    "nonhate_df['label'] = 0\n",
    "\n",
    "df = pd.concat([hate_df, nonhate_df], ignore_index=True)\n",
    "\n",
    "contexts = df['sentence'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "dataset = MyDataset(contexts, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "train_dataset = Subset(dataset, indices=range(0, train_size))\n",
    "val_dataset = Subset(dataset, indices=range(train_size, train_size + val_size))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize Model and Tokenizer\n",
    "# model_path = \"cardiffnlp/twitter-roberta-base-hate-latest\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "model_path = \"/data/shared/llama2/llama/7B-Chat/\"\n",
    "model = LlamaForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "# model = LlamaForSequenceClassification.from_pretrained(model_path, load_in_8bit=True).to(device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "lr=1e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
    "\n",
    "# Fine-tuning Loop\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "\n",
    "    # Training Loop\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        texts, labels = batch\n",
    "        texts = list(texts)\n",
    "        texts = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**texts, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "            texts, labels = batch\n",
    "            texts = list(texts)\n",
    "            texts = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            outputs = model(**texts, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Validation Loss: {val_loss / len(val_dataloader)}\")\n",
    "\n",
    "model_name = f\"{model_path}-lr{lr}-epoch{epoch}\"\n",
    "model.save_pretraine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db19841",
   "metadata": {},
   "source": [
    "# CardiffNLP Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b20ce06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /data/installation/anaconda3/envs/lora/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/installation/anaconda3/envs/lora/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /data/installation/anaconda3/envs/lora did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/data/installation/anaconda3/envs/lora/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "/data/installation/anaconda3/envs/lora/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "usage: ipykernel_launcher.py [-h] [--model_name MODEL_NAME]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--n_epoch N_EPOCH] [--model_type MODEL_TYPE]\n",
      "                             [--include] [--no-include]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jzheng36/.local/share/jupyter/runtime/kernel-a5f38b97-ca85-4db1-8b01-3c69a43a1e85.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/installation/anaconda3/envs/lora/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3556: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "\n",
    "def train_hate_model(args):\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    conan = pd.read_csv(\"/data/shared/hate_speech_dataset/CONAN.csv\")\n",
    "    hate_df = conan[['hateSpeech']].drop_duplicates()\n",
    "    hate_df = hate_df.rename(columns={'hateSpeech': 'sentence'})\n",
    "    hate_df = hate_df.reset_index(drop=True)\n",
    "    hate_df['labels'] = 1\n",
    "\n",
    "    nonhate_df = conan[['counterSpeech']].drop_duplicates()\n",
    "    nonhate_df = nonhate_df.rename(columns={'counterSpeech': 'sentence'})\n",
    "    nonhate_df = nonhate_df.reset_index(drop=True)\n",
    "    nonhate_df['labels'] = 0\n",
    "    train_df = pd.concat([hate_df, nonhate_df], ignore_index=True)\n",
    "    \n",
    "    compare_datasets = \"/data/jzheng36/hatemoderate/hatemoderate/fine_tune/cardiffnlp.pkl\"\n",
    "\n",
    "    model_args = ClassificationArgs()\n",
    "    model_args.learning_rate = args.learning_rate\n",
    "    model_args.num_train_epochs = args.n_epoch\n",
    "    model_args.train_batch_size = 32\n",
    "    model_args.eval_batch_size = 32\n",
    "    model_args.n_gpu = 4\n",
    "    model_args.output_dir = \"{}_lr={}_epoch={}_hatemoderate\".format(args.model_name.replace(\"/\", \"-\"), args.learning_rate, args.n_epoch)\n",
    "    model_args.overwrite_output_dir = True\n",
    "    model_args.save_best_model = True\n",
    "    model_args.use_multiprocessing = False\n",
    "    model_args.use_multiprocessing_for_evaluation = False\n",
    "    model_args.evaluate_during_training = False\n",
    "\n",
    "\n",
    "    model = ClassificationModel(args.model_type, args.model_name, num_labels=2, args=model_args)\n",
    "\n",
    "    cardiffnlp_datasets = pd.read_pickle(compare_datasets)\n",
    "    cardiffnlp_datasets = cardiffnlp_datasets.rename(columns={\"label\": \"labels\"})\n",
    "    cardiffnlp_datasets = cardiffnlp_datasets[cardiffnlp_datasets['split'] != 'test']\n",
    "\n",
    "    columns = [\"text\", \"labels\"]\n",
    "\n",
    "    train_df = train_df.rename(columns={\"sentence\": \"text\"}).sample(frac=1)\n",
    "    if args.include == True:\n",
    "        train_df = pd.concat([train_df[columns], cardiffnlp_datasets[columns]])\n",
    "    else:\n",
    "        train_df = pd.concat([cardiffnlp_datasets[columns]])\n",
    "\n",
    "\n",
    "\n",
    "    model.train_model(train_df=train_df)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train a hate speech classification model.\")\n",
    "    parser.add_argument(\"--model_name\", type=str, help=\"The name or path of the pre-trained model.\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-6, help=\"The learning rate to use for training. Default: 5e-6.\")\n",
    "    parser.add_argument(\"--n_epoch\", type=int, default=3, help=\"The number of epochs to train for. Default: 3.\")\n",
    "    parser.add_argument(\"--model_type\", type=str, default=\"roberta\", help=\"The type of the model (e.g., 'roberta', 'bert'). Default: 'roberta'.\")\n",
    "    parser.add_argument(\"--include\", action=\"store_true\", default=True, help=\"Whether to include the hatemoderate dataset in training. Default: True.\")\n",
    "    parser.add_argument(\"--no-include\", action=\"store_false\", dest=\"include\",\n",
    "                        help=\"Do not include the hatemoderate dataset in training.\")\n",
    "    args = parser.parse_args()\n",
    "    train_hate_model(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f973251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
